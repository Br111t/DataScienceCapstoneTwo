{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1 Contents<a id='4.1_Contents'></a>\n",
    "* [4 Pre-Processing and Training Data](#4_Pre-Processing_and_Training_Data)\n",
    "  * [4.1 Contents](#4.1_Contents)\n",
    "  * [4.2 Introduction](#4.2_Introduction)\n",
    "  * [4.3 Imports](#4.3_Imports)\n",
    "  * [4.4 Load Data](#4.4_Load_Data)\n",
    "  * [4.5 Choosing Models and Lags](#4.5_Choosing_Models_and_Lags)\n",
    "    * [4.5.1 Africa Subset COVID-19 Distribution](#4.5.1_Africa_Subset_COVID-19_Distribution)\n",
    "    * [4.5.2 Asia Subset COVID-19 Distribution](#4.5.2_Asia_Subset_COVID-19_Distribution)\n",
    "    * [4.5.3 Europe Subset COVID-19 Distribution](#4.5.3_Europe_Subset_COVID-19_Distribution)\n",
    "    * [4.5.4 E Subset COVID-19 Distribution](#4.5.4_Europe_Subset_COVID-19_Distribution)\n",
    "  * [4.6 Active and Recovered COVID-19 cases by Continent](#4.6_Active_and_Recovered_COVID-19_Cases_by_Continent)\n",
    "  * [4.6 Train/Test Split](#4.6_Train/Test_Split)\n",
    "  * [4.7 Initial Not-Even-A-Model](#4.7_Initial_Not-Even-A-Model)\n",
    "    * [4.7.1 Metrics](#4.7.1_Metrics)\n",
    "      * [4.7.1.1 R-squared, or coefficient of determination](#4.7.1.1_R-squared,_or_coefficient_of_determination)\n",
    "      * [4.7.1.2 Mean Absolute Error](#4.7.1.2_Mean_Absolute_Error)\n",
    "      * [4.7.1.3 Mean Squared Error](#4.7.1.3_Mean_Squared_Error)\n",
    "    * [4.7.2 sklearn metrics](#4.7.2_sklearn_metrics)\n",
    "        * [4.7.2.0.1 R-squared](#4.7.2.0.1_R-squared)\n",
    "        * [4.7.2.0.2 Mean absolute error](#4.7.2.0.2_Mean_absolute_error)\n",
    "        * [4.7.2.0.3 Mean squared error](#4.7.2.0.3_Mean_squared_error)\n",
    "    * [4.7.3 Note On Calculating Metrics](#4.7.3_Note_On_Calculating_Metrics)\n",
    "  * [4.8 Initial Models](#4.8_Initial_Models)\n",
    "    * [4.8.1 Imputing missing feature (predictor) values](#4.8.1_Imputing_missing_feature_(predictor)_values)\n",
    "      * [4.8.1.1 Impute missing values with median](#4.8.1.1_Impute_missing_values_with_median)\n",
    "        * [4.8.1.1.1 Learn the values to impute from the train set](#4.8.1.1.1_Learn_the_values_to_impute_from_the_train_set)\n",
    "        * [4.8.1.1.2 Apply the imputation to both train and test splits](#4.8.1.1.2_Apply_the_imputation_to_both_train_and_test_splits)\n",
    "        * [4.8.1.1.3 Scale the data](#4.8.1.1.3_Scale_the_data)\n",
    "        * [4.8.1.1.4 Train the model on the train split](#4.8.1.1.4_Train_the_model_on_the_train_split)\n",
    "        * [4.8.1.1.5 Make predictions using the model on both train and test splits](#4.8.1.1.5_Make_predictions_using_the_model_on_both_train_and_test_splits)\n",
    "        * [4.8.1.1.6 Assess model performance](#4.8.1.1.6_Assess_model_performance)\n",
    "      * [4.8.1.2 Impute missing values with the mean](#4.8.1.2_Impute_missing_values_with_the_mean)\n",
    "        * [4.8.1.2.1 Learn the values to impute from the train set](#4.8.1.2.1_Learn_the_values_to_impute_from_the_train_set)\n",
    "        * [4.8.1.2.2 Apply the imputation to both train and test splits](#4.8.1.2.2_Apply_the_imputation_to_both_train_and_test_splits)\n",
    "        * [4.8.1.2.3 Scale the data](#4.8.1.2.3_Scale_the_data)\n",
    "        * [4.8.1.2.4 Train the model on the train split](#4.8.1.2.4_Train_the_model_on_the_train_split)\n",
    "        * [4.8.1.2.5 Make predictions using the model on both train and test splits](#4.8.1.2.5_Make_predictions_using_the_model_on_both_train_and_test_splits)\n",
    "        * [4.8.1.2.6 Assess model performance](#4.8.1.2.6_Assess_model_performance)\n",
    "    * [4.8.2 Pipelines](#4.8.2_Pipelines)\n",
    "      * [4.8.2.1 Define the pipeline](#4.8.2.1_Define_the_pipeline)\n",
    "      * [4.8.2.2 Fit the pipeline](#4.8.2.2_Fit_the_pipeline)\n",
    "      * [4.8.2.3 Make predictions on the train and test sets](#4.8.2.3_Make_predictions_on_the_train_and_test_sets)\n",
    "      * [4.8.2.4 Assess performance](#4.8.2.4_Assess_performance)\n",
    "  * [4.9 Refining The Linear Model](#4.9_Refining_The_Linear_Model)\n",
    "    * [4.9.1 Define the pipeline](#4.9.1_Define_the_pipeline)\n",
    "    * [4.9.2 Fit the pipeline](#4.9.2_Fit_the_pipeline)\n",
    "    * [4.9.3 Assess performance on the train and test set](#4.9.3_Assess_performance_on_the_train_and_test_set)\n",
    "    * [4.9.4 Define a new pipeline to select a different number of features](#4.9.4_Define_a_new_pipeline_to_select_a_different_number_of_features)\n",
    "    * [4.9.5 Fit the pipeline](#4.9.5_Fit_the_pipeline)\n",
    "    * [4.9.6 Assess performance on train and test data](#4.9.6_Assess_performance_on_train_and_test_data)\n",
    "    * [4.9.7 Assessing performance using cross-validation](#4.9.7_Assessing_performance_using_cross-validation)\n",
    "    * [4.9.8 Hyperparameter search using GridSearchCV](#4.9.8_Hyperparameter_search_using_GridSearchCV)\n",
    "  * [4.10 Random Forest Model](#4.10_Random_Forest_Model)\n",
    "    * [4.10.1 Define the pipeline](#4.10.1_Define_the_pipeline)\n",
    "    * [4.10.2 Fit and assess performance using cross-validation](#4.10.2_Fit_and_assess_performance_using_cross-validation)\n",
    "    * [4.10.3 Hyperparameter search using GridSearchCV](#4.10.3_Hyperparameter_search_using_GridSearchCV)\n",
    "  * [4.11 Final Model Selection](#4.11_Final_Model_Selection)\n",
    "    * [4.11.1 Linear regression model performance](#4.11.1_Linear_regression_model_performance)\n",
    "    * [4.11.2 Random forest regression model performance](#4.11.2_Random_forest_regression_model_performance)\n",
    "    * [4.11.3 Conclusion](#4.11.3_Conclusion)\n",
    "  * [4.12 Data quantity assessment](#4.12_Data_quantity_assessment)\n",
    "  * [4.13 Save best model object from pipeline](#4.13_Save_best_model_object_from_pipeline)\n",
    "  * [4.14 Summary](#4.14_Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series analysis has some things in common with discussion of probability and regression\n",
    "* In many cases, we can describe parts of time series processes in terms of a randomized variable with statistical moments, though an important feature of many time series processes is that their mean and/or variance change through time. \n",
    "* Like regression, time series analysis is often focused on identifying underlying trends and patterns, describing them mathematically, and ultimately making a prediction or forecast about what will happen next.  \n",
    "In this instance the data can be modeled to identify the trends for active and recovered patients. A big part of this time series analysis will involve filtering - i.e. changing attributes of a time series or deconstructing it into it's component parts.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels import __version__ as statsmodels_version\n",
    "import pickle\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Load Data<a id='4.4_Load_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_df = pd.read_csv('../data/mor_df.csv',  index_col=0)\n",
    "asia_df = pd.read_csv('../data/asia_df.csv',  index_col=0)\n",
    "eu_df = pd.read_csv('../data/eu_df.csv',  index_col=0)\n",
    "noam_df = pd.read_csv('../data/noam_df.csv',  index_col=0)\n",
    "soam_df = pd.read_csv('../data/soam_df.csv',  index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Choosing Models and Lags<a id='4.5_Choosing_Models_and_Lags'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Augmented dickey Fuller Test (ADF) is a unit root test for stationarity. Unit roots can cause unpredictable results in your time series analysis, it should be used with caution because it has a high Type I error rate. The type I error (false positive). So in this case it would be identifying more recovered and active cases per country, a cautious approach to disease transmission. \n",
    "\n",
    "Before running the ADF test, the data must be inspected to figure out an appropriate regression model. A nonzero mean indicates the regression will have a constant term. The three basic regression models are: \n",
    "\n",
    "* No constant, no trend: $ \\Delta y_t = \\gamma y_{t-1} + v_t $\n",
    "* Constant, no trend: $ \\Delta y_t = \\alpha + \\gamma y_{t-1} + v_t $\n",
    "* Constant and trend: $ \\Delta y_t = \\alpha + \\gamma y_{t-1} + \\lambda_t + v_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Africa Subset COVID-19 Data Distribution<a id='4.5.1_Africa_Subset_COVID-19_Data_Distribution'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "af_df['date'] = pd.to_datetime(af_df['date'])\n",
    "af = af_df[['date','active','recovered']].groupby(pd.Grouper(key='date', axis=0, freq='1D', sort=True)).sum()\n",
    "af[180:220] = af[180:220].replace(to_replace=0, method='ffill')\n",
    "af.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Asia Subset COVID-19 Data Distribution<a id='4.5.2_Asia_Subset_COVID-19_Data_Distribution'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "asia_df['date'] = pd.to_datetime(asia_df.index)\n",
    "asia = asia_df[['date','active','recovered']].groupby(pd.Grouper(key='date', axis=0, freq='1D', sort=True)).sum()\n",
    "asia[220:280] = asia[220:280].replace(to_replace=0, method='ffill')\n",
    "asia.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Europe Subset COVID-19 Data Distribution<a id='4.5.3_Europe_Subset_COVID-19_Data_Distribution'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eu_df['date'] = pd.to_datetime(eu_df.index)\n",
    "eu = eu_df[['date','active','recovered']].groupby(pd.Grouper(key='date', axis=0, freq='1D', sort=True)).sum()\n",
    "eu[210:250] = eu[210:250].replace(to_replace=0, method='ffill')\n",
    "eu.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 North America Subset COVID-19 Data Distribution<a id='4.5.4_North_America_Subset_COVID-19_Data_Distribution'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noam_df['date'] = pd.to_datetime(noam_df.index)\n",
    "noam = noam_df[['date','active','recovered']].groupby(pd.Grouper(key='date', axis=0, freq='1D', sort=True)).sum()\n",
    "noam[215:255] = noam[215:255].replace(to_replace=0, method='ffill')\n",
    "noam.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 South America Subset COVID-19 Data Distribution<a id='4.5.5_South_America_Subset_COVID-19_Data_Distribution'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soam_df['date'] = pd.to_datetime(soam_df.index)\n",
    "soam = soam_df[['date','active','recovered']].groupby(pd.Grouper(key='date', axis=0, freq='1D', sort=True)).sum()\n",
    "soam[215:255] = soam[215:255].replace(to_replace=0, method='ffill')\n",
    "soam.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Active and Recovered COVID-19 cases by Continent<a id='4.6_Active_and_Recovered_COVID-19_Cases_by_Continent'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 2, figsize=(14, 14))\n",
    "\n",
    "# Define the date format\n",
    "date_form = DateFormatter(\"%m-%d\")\n",
    "\n",
    "# plotting subplots\n",
    "sns.lineplot(ax=axes[0,0], data=noam, x='date', y='active', ci=95)\n",
    "axes[0, 0].set_title('North America Subset COVID-19 Cases 2020', loc='left', pad=15)\n",
    "axes[0, 0].margins(x=0)\n",
    "axes[0, 0].xaxis.set_major_formatter(date_form)\n",
    "axes[0, 0].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[0,1], data=noam, x='date', y='recovered', ci=95)\n",
    "axes[0, 1].margins(x=0)\n",
    "axes[0, 1].xaxis.set_major_formatter(date_form)\n",
    "axes[0, 1].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[1,0], data=asia, x='date', y='active', ci=95)\n",
    "axes[1, 0].set_title('Asia Subset COVID-19 Cases 2020', loc='left', pad=15)\n",
    "axes[1, 0].margins(x=0)\n",
    "axes[1, 0].xaxis.set_major_formatter(date_form)\n",
    "axes[1, 0].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[1, 1], data=asia, x='date', y='recovered', ci=95)\n",
    "axes[1, 1].margins(x=0)\n",
    "axes[1, 1].xaxis.set_major_formatter(date_form)\n",
    "axes[1, 1].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[2, 0], data=eu, x='date', y='active', ci=95)\n",
    "axes[2, 0].set_title('Europe Subset COVID-19 Cases 2020', loc='left', pad=15)\n",
    "axes[2, 0].margins(x=0)\n",
    "axes[2, 0].xaxis.set_major_formatter(date_form)\n",
    "axes[2, 0].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[2, 1], data=eu, x='date', y='recovered', ci=95)\n",
    "axes[2, 1].margins(x=0)\n",
    "axes[2, 1].xaxis.set_major_formatter(date_form)\n",
    "axes[2, 1].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[3, 0], data=af, x='date', y='active', ci=95)\n",
    "axes[3, 0].set_title('Africa Subset COVID-19 Cases 2020', loc='left', pad=15)\n",
    "axes[3, 0].margins(x=0)\n",
    "axes[3, 0].xaxis.set_major_formatter(date_form)\n",
    "axes[3, 0].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[3, 1], data=af, x='date', y='recovered', ci=95)\n",
    "axes[3, 1].margins(x=0)\n",
    "axes[3, 1].xaxis.set_major_formatter(date_form)\n",
    "axes[3, 1].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[4, 0], data=soam, x='date', y='active', ci=95)\n",
    "axes[4, 0].set_title('Africa Subset COVID-19 Cases 2020', loc='left', pad=15)\n",
    "axes[4, 0].margins(x=0)\n",
    "axes[4, 0].xaxis.set_major_formatter(date_form)\n",
    "axes[4, 0].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "sns.lineplot(ax=axes[4, 1], data=soam, x='date', y='recovered', ci=95)\n",
    "axes[4, 1].margins(x=0)\n",
    "axes[4, 1].xaxis.set_major_formatter(date_form)\n",
    "axes[4, 1].xaxis.set_major_locator(mdates.WeekdayLocator(interval=5))\n",
    "\n",
    "\n",
    "# automatically adjust padding horizontally\n",
    "# as well as vertically.\n",
    "plt.tight_layout()\n",
    " \n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a p-value of less than 5% means you can reject the null hyptohesis that there is a unit root. In this case we want to ensure that there are no values that can lead to unpredictable results in the time series. The alternate hypothesis for this data is that the time series *IS* stationary (or trend stationary) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Africa ADF Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(af['recovered'])\n",
    "result1 = adfuller(af['active'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Africa Sample Recovered ADF Statistic:', result[0])\n",
    "\n",
    "print('Africa Sample Recovered p-value:', result[1])\n",
    "\n",
    "print('Africa Sample Recovered critical values:', result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Africa Sample Active ADF Statistic:', result1[0])\n",
    "\n",
    "print('Africa Sample Active p-value:', result1[1])\n",
    "\n",
    "print('Africa Sample Active critical values:', result1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_active = af.active.diff().diff().dropna()\n",
    "result2 = adfuller(af_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Africa Sample Active ADF Statistic:', result2[0])\n",
    "\n",
    "print('Africa Sample Active p-value:', result2[1])\n",
    "\n",
    "print('Africa Sample Active critical values:', result2[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia ADF statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia_recovered = asia.recovered.diff().diff().dropna()\n",
    "result3 = adfuller(asia_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Asia Sample Recovered ADF Statistic:', result3[0])\n",
    "\n",
    "print('Asia Sample Recovered p-value:', result3[1])\n",
    "\n",
    "print('Asia Sample Recovered critical values:', result3[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia_active = asia.active.diff().dropna()\n",
    "result4 = adfuller(asia_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Asia Sample Active ADF Statistic:', result4[0])\n",
    "\n",
    "print('Asia Sample Active p-value:', result4[1])\n",
    "\n",
    "print('Asia Sample Active critical values:', result4[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Europe ADF Statisic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_recovered = eu.recovered.diff().dropna()\n",
    "result5 = adfuller(eu_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Europe Sample Recovered ADF Statistic:', result5[0])\n",
    "\n",
    "print('Europe Sample Recovered p-value:', result5[1])\n",
    "\n",
    "print('Europe Sample Recovered critical values:', result5[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_active = eu.active.diff().diff().dropna()\n",
    "result6 = adfuller(eu_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Europe Sample Active ADF Statistic:', result6[0])\n",
    "\n",
    "print('Europe Sample Active p-value:', result6[1])\n",
    "\n",
    "print('Europe Sample Active critical values:', result6[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#North America ADF Statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noam_recovered = noam_df.recovered.diff().dropna()\n",
    "result7 = adfuller(noam_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('North America Sample Recovered ADF Statistic:', result7[0])\n",
    "\n",
    "print('North America Sample Recovered p-value:', result7[1])\n",
    "\n",
    "print('North America Sample Recovered critical values:', result7[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noam_active = noam_df.active.diff().dropna()\n",
    "result8 = adfuller(noam_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('North America Sample Active ADF Statistic:', result8[0])\n",
    "\n",
    "print('North America Sample Active p-value:', result8[1])\n",
    "\n",
    "print('North America Sample Active critical values:', result8[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating grid for subplots\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax1 = plt.subplot2grid(shape=(2, 1), loc=(0, 0))\n",
    "ax2 = plt.subplot2grid(shape=(2, 1), loc=(1, 0))\n",
    "\n",
    "# plotting subplots\n",
    "ax1.plot(noam_recovered)\n",
    "ax1.set_title('North America Recovered MAR 2020 - DEC 2020')\n",
    "ax2.plot(noam_active)\n",
    "ax2.set_title('North America Active MAR 2020 - DEC 2020') \n",
    "\n",
    "# automatically adjust padding horizontally\n",
    "# as well as vertically.\n",
    "plt.tight_layout()\n",
    "\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soam_df = soam_df.groupby('date')[['confirmed', 'deaths', 'recovered', 'active']].sum()\n",
    "soam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soam_recovered = soam_df.recovered.diff().dropna()\n",
    "result8 = adfuller(soam_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('South America Sample Recovered ADF Statistic:', result8[0])\n",
    "\n",
    "print('South America Sample Recovered p-value:', result8[1])\n",
    "\n",
    "print('South America Sample Recovered critical values:', result8[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soam_active = soam_df.active.diff().dropna()\n",
    "result9 = adfuller(soam_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('South America Sample Recovered ADF Statistic:', result9[0])\n",
    "\n",
    "print('South America Sample Recovered p-value:', result9[1])\n",
    "\n",
    "print('South America Sample Recovered critical values:', result9[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating grid for subplots\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax1 = plt.subplot2grid(shape=(2, 1), loc=(0, 0))\n",
    "ax2 = plt.subplot2grid(shape=(2, 1), loc=(1, 0))\n",
    "\n",
    "# plotting subplots\n",
    "ax1.plot(soam_recovered)\n",
    "ax1.set_title('South America Recovered MAR 2020 - DEC 2020')\n",
    "ax2.plot(soam_active)\n",
    "ax2.set_title('South America Active MAR 2020 - DEC 2020') \n",
    "\n",
    "# automatically adjust padding horizontally\n",
    "# as well as vertically.\n",
    "plt.tight_layout()\n",
    "\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After graphing the difference for a stationary model determining ARMA, AR or MA \n",
    "\n",
    "ACF and PACF cannot be used to choose the order of a model when both the orders q and p are non-zero. Instead there are other models the AIC and BIC.\n",
    "\n",
    "    a. The model with the lower AIC Score makes better predictions \n",
    "    \n",
    "\n",
    "If you recieve a value error this is a bad model for the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Dickey Fuller adds lagged differences to these models\n",
    "\n",
    "You must choose a lag length to run the test. The lag length should be chosen so that the residuals aren't serially correlated. To choose the lags: minimize Akaike's information criterion will be used. \n",
    "\n",
    "The AIC deals with both the risk of overfitting and the risk of underfitting. \n",
    "\n",
    "A p-values of less than 5% means you can reject the null hypothesis that there is a unit root. You can also compare the calculated DF_t statistic with a tabulated critical value. The more negative the DF test statistic , the stronger the evidence for rejecting the null hypothesis of a unit root \n",
    "\n",
    "The DF test statistic: (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Description of AR(1) Model \n",
    "R_t = mu + phi(R_t-1) + epselon_t\n",
    "\n",
    "Since there is only one lagged value on the right hand side, \n",
    "this is called an AR model of order 1, or simply an AR(1) model. Or simply \n",
    "An AR(1) model  \n",
    "*AR parameter is phi\n",
    "    If phi is 1 then the process is a random walk \n",
    "    If phi is 0, then the process is white noise\n",
    "\n",
    "In order for the process to be stable and stationary, phi has to be between \n",
    "-1 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arica data\n",
    "log_af_rec = np.log(af_df['recovered'])\n",
    "log_af_rec = log_af_rec.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_af_rec.to_csv(\"../data/log_af_rec.csv\")\n",
    "log_af_act = np.log(af_df['active'])\n",
    "log_af_act = log_af_act.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_af_act.to_csv(\"../data/log_af_act.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asia data\n",
    "log_asia_rec = np.log(asia_df['recovered'])\n",
    "log_asia_rec = log_asia_rec.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_asia_rec.to_csv(\"../data/log_asia_rec.csv\")\n",
    "log_asia_act = np.log(asia_df['active'])\n",
    "log_asia_act = log_asia_act.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_asia_act.to_csv(\"../data/log_asia_act.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Europe data \n",
    "log_eu_rec = np.log(eu_df['recovered'])\n",
    "log_eu_rec = log_eu_rec.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_eu_rec.to_csv(\"../data/log_eu_rec.csv\")\n",
    "log_eu_act = np.log(eu_df['active'])\n",
    "log_eu_act = log_eu_act.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_eu_act.to_csv(\"../data/log_eu_act.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#North America data\n",
    "log_noam_rec = np.log(noam_df['recovered'])\n",
    "log_noam_rec = log_noam_rec.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_noam_rec.to_csv(\"../data/log_noam_rec.csv\")\n",
    "log_noam_act = np.log(noam_df['active'])\n",
    "log_noam_act = log_noam_act.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_noam_act.to_csv(\"../data/log_noam_act.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#South America data\n",
    "log_soam_rec = np.log(soam_df['recovered'])\n",
    "log_soam_rec = log_soam_rec.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_soam_rec.to_csv(\"../data/log_soam_rec.csv\")\n",
    "log_soam_act = np.log(soam_df['active'])\n",
    "log_soam_act = log_soam_act.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "log_soam_act.to_csv(\"../data/log_soam_act.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Function modified from Cowboy Cigarettes Case Study \n",
    "\n",
    "# Make a function called evaluate_arima_model to find the MSE of a single ARIMA model \n",
    "def evaluate_arima_model(data, arima_order):\n",
    "    # Needs to be an integer because it is later used as an index.\n",
    "    # Use int()\n",
    "    split=int(len(data) * 0.8) \n",
    "    # Make train and test variables, with 'train, test'\n",
    "    train, test = data[0:split], data[split:len(data)]\n",
    "    past=[x for x in train]\n",
    "    # make predictions. Declare a variable with that name\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):#timestep-wise comparison between test data and one-step prediction ARIMA model. \n",
    "        model = ARIMA(past, order=arima_order)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        future = model_fit.forecast()[0]\n",
    "        # Append() here\n",
    "        predictions.append(future)\n",
    "        past.append(test[i])\n",
    "    # calculate out of sample error\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    # Return the error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Function modified from Cowboy Cigarettes Case Study \n",
    "\n",
    "# Make a function called evaluate_models to evaluate different ARIMA models with several different p, d, and q values.\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    # Iterate through p_values\n",
    "    for p in p_values:\n",
    "        # Iterate through d_values\n",
    "        for d in d_values:\n",
    "            # Iterate through q_values\n",
    "            for q in q_values:\n",
    "                # p, d, q iterator variables in that order\n",
    "                order = (p, d, q)\n",
    "                try:\n",
    "                    # Make a variable called mse for the Mean squared error\n",
    "                    mse = evaluate_arima_model(dataset, order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    return print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we choose a couple of values to try for each parameter: p_values, d_values and q_values\n",
    "# Fill in the blanks as appropriate\n",
    "p_values = [x for x in range(0, 3)]\n",
    "d_values = [x for x in range(0, 3)]\n",
    "q_values = [x for x in range(0, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monkey patch around bug in ARIMA class\n",
    "# def __getnewargs__(self):\n",
    "# \treturn ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "# ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pickle_best_model(data, arima_order, filename):\n",
    "\n",
    "    # Save Model Using Pickle\n",
    "    split=int(len(data) * 0.8) \n",
    "\n",
    "    # Make train and test variables, with 'train, test'\n",
    "    train, test = data[0:split], data[split:len(data)]\n",
    "    past=[x for x in train]\n",
    "    # Fit the model on training set with best order\n",
    "    model = ARIMA(past, order=arima_order)\n",
    "    model_fit = model.fit(disp=0)\n",
    "    model_fit.version = '1.0'\n",
    "    model_fit.pandas_version = pd.__version__\n",
    "    model_fit.numpy_version = np.__version__\n",
    "    model_fit.sklearn_version = sklearn_version\n",
    "    model_fit.statsmodels_version = statsmodels_version\n",
    "    model_fit.build_datetime = datetime.datetime.now()\n",
    "    \n",
    "    # save the model to disk - write binary\n",
    "    ##filename = 'finalized_model.sav'\n",
    "    pickle.dump(model_fit, open(filename, 'wb'))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our Africa recovered sample.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(log_af_rec, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_af_rec, (2, 2, 0), '../models/af_rec_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our Africa active sample.\n",
    "evaluate_models(log_af_act, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_af_act, (0, 2, 1), '../models/af_act_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our Asia recovered sample.\n",
    "evaluate_models(log_asia_rec, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_asia_rec, (1, 1, 1), '../models/asia_rec_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our Asia active sample.\n",
    "evaluate_models(log_asia_act, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_asia_act, (1, 0, 0), '../models/asia_act_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our Europe recovered sample.\n",
    "evaluate_models(log_eu_rec, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_eu_rec, (1, 2, 0), '../models/eu_rec_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our Europe active sample.\n",
    "evaluate_models(log_eu_act, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_eu_act, (2, 2, 0), '../models/eu_act_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our North America recovered sample.\n",
    "evaluate_models(log_noam_rec, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_noam_rec, (1, 0, 0), '../models/noam_rec_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our North America active sample.\n",
    "evaluate_models(log_noam_act, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_noam_act, (0, 1, 0), '../models/noam_act_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our South America recovered sample.\n",
    "evaluate_models(log_soam_rec, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_soam_rec, (2, 2, 0), '../models/soam_rec_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum ARIMA model for our South America active sample.\n",
    "evaluate_models(log_soam_act, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_best_model(log_soam_act, (1, 0, 0), '../models/soam_act_model.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
